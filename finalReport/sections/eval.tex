\section{Evaluation}

In order to evaluate this technique, 1 hour of location data was captured for 4 busses in the Edmonton Public Transit System. This data consists of 120 sensor updates per bus, or 1 every 30 seconds. All results were captured on a machine with 32 GB of RAM, and a Core i5 2700K processor.

Unless otherwise specified, all probability meshes are 256*256*9*9, and are mapped to the Edmonton City Limits, with velocities in the range (-100km/h,100km/h).

\subsection{Optimal Grid Size}
Using a finer spatial grid ($n$ and $m$) allows for smaller changes in position to be represented, and for finer points to be selected as most-likely positions.
However, in order to be useful for real-time refinement, it must be possible to update meshes at least as quickly as new sensor updates come in. This means it must be possible to update all meshes in under 1 hour.

By calculating recoverability between the raw GPS positions and the refined positions for a series of grid sizes, we can determine at what grid size the fidelity no longer contributes a substantial error. Because in this case refinement itself will appear to contribute error, we are looking not for a low absolute error, but a "levelling off" in which increasing size does not further reduce error.
Error is presented here as the mean distance between points at the same timestamp, in meters.

\input{figures/fig-size-err}

Shown in \figref{fig:size:err}, substantial error is contributed by the 64x64 and 128x128 grids, but beyond that there appears to be negligible gain from further size increases. It may still be desirable to use larger grids, as the error continues to reduce, but execution time may prevent it. \figref{fig:size:time} shows the time required to compute all 120 sensor updates for all 4 busses. While sizes up to 256x256 are relatively efficient to compute, it takes nearly the full hour to process the 1024x1024 grid.

\input{figures/fig-size-time}

\subsection{Route Recoverability}

Because GPS sensors are assumed in this work to be unreliable, and the resulting techniques operate on that assumption, it makes little sense to compare the refined paths to the original sensor data. Instead, we perform multiple runs, randomly permuting GPS positions within specified radii. By comparing the error from raw sensor input to the error in the refined path, we can evaluate the effectiveness of these techniques in accomodating for errors. \figref{fig:gps:err} shows the resulting information.

\input{figures/fig-gps-err}

Clearly visible in the figure, path refinement fails at all error levels to produce a more recoverable path than just using the original sensor readings. However, as GPS error increases, the gap begins to close in relative terms. At 10cm of GPS error, the refined error is 8.4x greater, while at 2m of GPS error, the refined error is only 3x greater. At further inspection, this error occurs for two main reasons:
\begin{enumerate}
    \item \textbf{Error Magnification:} The probability meshes can express maximum likelihood points only at exact grid cells. Thus, a small change in raw position can cause the refined path to move the position an entire cell. Since cells correspond to positions multiple meters apart, this magnifies small changes into large changes. Though most cells are unchanged, a few errors skew the result.

    \item \textbf{Off-the-grid Error:} One bus being tracked actually leaves the City of Edmonton geographic limits for a portion of its route. During this portion, the probability mesh is forced to approximate its location as at the nearest edge.
\end{enumerate}

These issues, along with the reducing gap in recoverability as the error grows more dramatic, seem to indicate that a more precise probability mesh would solve much of the issue in refined recoverability.

%\input{figures/fig-algm-ex}
%\input{figures/fig-code-ex}
%\input{figures/fig-tbl-ex}
